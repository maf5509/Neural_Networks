{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Data Analysis Using the VGG16 Model\n",
    "\n",
    "This notebook is a continuation of a project to perform a neural network analysis on a graphical dataset. The target dataset is a well-known collection of images known as the CIFAR-10, which is available for download at the following link:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">(http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical report is shown here:\n",
    "<div class=\"alert alert-block alert-success\">(http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task was the train a neural network to be able to identify images, split into 10 classes, based on a training dataset of size 50,000 and a test dataset of 10,000.\n",
    "\n",
    "The initial analysis was done using first the Multi-Layer Perceptron model. This was followed by a conventional Convolutional Neural Network (CNN) analysis. Owing to the limited CPU resources available, initial runs had to be done on only a subset of the dataset. Once the right modifications had been made to the model, each analysis could be done on the entire dataset. See the 'Results' section below for an account of the GPU-assisted runs and the consequent speed improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Accuracy Achieved: 82.7%\n",
    "\n",
    "In addition to the accuracy, a major performance-related aspect of this mini-project has been runtime. After the CNN runs (with and wihout data augmentation) had taken an inordinate amount of time, the decision was made to switch to using Google colabs, which give the option to run Jupyter Notebooks using GPUs. This step greatly boosted performance and slashed runtimes. Below is a summary of the results for each method:\n",
    "\n",
    "<BR>\n",
    "\n",
    "\n",
    "| Model | Full Dataset? | Data Augmented  |     Layers    |  Epochs  | Time Taken | Accuracy |\n",
    "|-------|---------------|-----------------|---------------|----------|------------|----------|\n",
    "|MLP    |   Yes         |  No             |      2        |    50    |   10.5 min | 0.486    |\n",
    "|CNN    |  Yes          |  No             |    1 hidden   |    10    |  3.7 hr    | 0.701    |\n",
    "|CNN    |  Yes          |  No             |    3 hidden   |    50    |  18.3 hr   | 0.732    |\n",
    "|CNN    |  Yes          |  Yes            |    3 hidden   |    50    |  23.5 hr   | 0.713    |\n",
    "|VGG16  |  Yes          |  No             |    16         |    50    |  52.6 min  | 0.798    |\n",
    "|VGG16  |  Yes          |  Yes            |    16         |    50    |  55.8 min  | 0.827    |\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4HENNMpkOKfy",
    "outputId": "3bab5211-627b-424a-aefb-905010560f5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6jyN-OCmOrKo",
    "outputId": "47a28455-34f8-41eb-a441-ebe18d6a795e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 36s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ALhLVs-0PD7z",
    "outputId": "85c79807-ed70-4442-d496-636b1e8d9a84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3ynkXWfSPryk"
   },
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ati-K2VVPwUx"
   },
   "outputs": [],
   "source": [
    "# Set image size for VGG Model\n",
    "img_rows, img_cols = 32,32\n",
    "# Set number of output classes\n",
    "num_classes = 10\n",
    "# Set proportion of entire dataset to be used (e.g. 0.01 = 1%)\n",
    "prop = 1 # Here we use the full dataset\n",
    "size1 = int(len(x_train) * prop)\n",
    "size2 = int(len(x_test) * prop)\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = x_train[:size1], x_test[:size2], y_train[:size1], y_test[:size2]\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_tr = x_tr.reshape(x_tr.shape[0], 3, 32, 32)\n",
    "    x_te = x_te.reshape(x_te.shape[0], 3, 32, 32)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_tr = x_tr.reshape(x_tr.shape[0], 32, 32, 3)\n",
    "    x_te = x_te.reshape(x_te.shape[0], 32, 32, 3)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_tr = keras.utils.to_categorical(y_tr, num_classes)\n",
    "y_te = keras.utils.to_categorical(y_te, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1885
    },
    "colab_type": "code",
    "id": "534Pef_6Ba1Y",
    "outputId": "6f71c2a4-c100-4e01-b814-a3389fe7588d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to add hidden layers. Time taken: 7.232030232747396e-07 minutes\n",
      "About to add hidden layers. Time taken: 0.023129260540008544 minutes\n",
      "Layer-adding half done. Time taken: 0.040234665075937905 minutes\n",
      "Layer-adding done, about to compile. Time taken: 0.053899558385213216 minutes\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 75s 1ms/step - loss: 4.3427 - acc: 0.2481 - val_loss: 4.7186 - val_acc: 0.2119\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 3.5390 - acc: 0.3855 - val_loss: 4.1945 - val_acc: 0.2935\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 2.9782 - acc: 0.4821 - val_loss: 2.9060 - val_acc: 0.4912\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 2.3697 - acc: 0.5956 - val_loss: 3.1476 - val_acc: 0.4484\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 2.0331 - acc: 0.6471 - val_loss: 2.0945 - val_acc: 0.5863\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.7298 - acc: 0.6967 - val_loss: 2.2011 - val_acc: 0.5769\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.5130 - acc: 0.7332 - val_loss: 1.9276 - val_acc: 0.5909\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.3519 - acc: 0.7606 - val_loss: 1.6995 - val_acc: 0.6556\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.2524 - acc: 0.7772 - val_loss: 2.0311 - val_acc: 0.5597\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.1613 - acc: 0.7953 - val_loss: 1.4250 - val_acc: 0.7136\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.1149 - acc: 0.8028 - val_loss: 1.2685 - val_acc: 0.7508\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.0594 - acc: 0.8175 - val_loss: 1.3405 - val_acc: 0.7359\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.0363 - acc: 0.8229 - val_loss: 1.2666 - val_acc: 0.7474\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.0053 - acc: 0.8324 - val_loss: 1.4492 - val_acc: 0.6941\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9822 - acc: 0.8392 - val_loss: 1.2085 - val_acc: 0.7806\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9756 - acc: 0.8452 - val_loss: 1.4302 - val_acc: 0.7120\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9634 - acc: 0.8523 - val_loss: 1.5395 - val_acc: 0.6924\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9517 - acc: 0.8578 - val_loss: 1.3476 - val_acc: 0.7442\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9503 - acc: 0.8605 - val_loss: 1.3927 - val_acc: 0.7335\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9444 - acc: 0.8665 - val_loss: 1.2931 - val_acc: 0.7638\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9464 - acc: 0.8695 - val_loss: 1.3761 - val_acc: 0.7537\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9495 - acc: 0.8692 - val_loss: 1.2559 - val_acc: 0.7790\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9391 - acc: 0.8757 - val_loss: 1.5947 - val_acc: 0.6840\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9434 - acc: 0.8775 - val_loss: 1.1363 - val_acc: 0.8125\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9352 - acc: 0.8813 - val_loss: 1.9910 - val_acc: 0.6237\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9387 - acc: 0.8819 - val_loss: 1.4508 - val_acc: 0.7623\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9394 - acc: 0.8849 - val_loss: 1.3815 - val_acc: 0.7552\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9364 - acc: 0.8874 - val_loss: 1.4276 - val_acc: 0.7550\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9383 - acc: 0.8879 - val_loss: 1.2646 - val_acc: 0.7937\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9450 - acc: 0.8880 - val_loss: 1.6396 - val_acc: 0.7074\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9433 - acc: 0.8902 - val_loss: 1.6773 - val_acc: 0.7157\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9430 - acc: 0.8914 - val_loss: 1.3872 - val_acc: 0.7749\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9415 - acc: 0.8934 - val_loss: 1.2017 - val_acc: 0.8173\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9503 - acc: 0.8930 - val_loss: 1.9544 - val_acc: 0.6549\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 0.9430 - acc: 0.8967 - val_loss: 1.2307 - val_acc: 0.8138\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9411 - acc: 0.8965 - val_loss: 1.3250 - val_acc: 0.7981\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9406 - acc: 0.8968 - val_loss: 1.3002 - val_acc: 0.8025\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9448 - acc: 0.8984 - val_loss: 1.1747 - val_acc: 0.8395\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9443 - acc: 0.8992 - val_loss: 1.2620 - val_acc: 0.8092\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9378 - acc: 0.9020 - val_loss: 1.8447 - val_acc: 0.6874\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9420 - acc: 0.9005 - val_loss: 1.1449 - val_acc: 0.8444\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9418 - acc: 0.9034 - val_loss: 1.2310 - val_acc: 0.8211\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9415 - acc: 0.9028 - val_loss: 1.1377 - val_acc: 0.8445\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9412 - acc: 0.9024 - val_loss: 1.3782 - val_acc: 0.7812\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9322 - acc: 0.9050 - val_loss: 1.6394 - val_acc: 0.7160\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9385 - acc: 0.9046 - val_loss: 2.4982 - val_acc: 0.5363\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9364 - acc: 0.9059 - val_loss: 1.1615 - val_acc: 0.8422\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9342 - acc: 0.9066 - val_loss: 1.3804 - val_acc: 0.7916\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9266 - acc: 0.9097 - val_loss: 1.2141 - val_acc: 0.8294\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.9374 - acc: 0.9064 - val_loss: 1.3380 - val_acc: 0.7978\n",
      "10000/10000 [==============================] - 8s 813us/step\n",
      "Test loss: 1.3380369396209717\n",
      "Test accuracy: 0.7978\n",
      "************************************************************\n",
      "Time taken: 52.6 minutes (0.9 hours).\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('About to add hidden layers. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "# Create and instantiate VGG model:\n",
    "model = Sequential()\n",
    "weight_decay = 0.0005\n",
    "input_shape = [32,32,3]\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=input_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "print('About to add hidden layers. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "print('Layer-adding half done. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "print('Layer-adding done, about to compile. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Score trained model.\n",
    "model.fit(x_tr, y_tr,\n",
    "          batch_size=256,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(x_te, y_te))\n",
    "score = model.evaluate(x_te, y_te, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "print('*' * 60)\n",
    "time_taken = '%.1f' % ((time.time() - start_time) / 60)\n",
    "time_hours = '%.1f' % ((time.time() - start_time) / 3600)\n",
    "print('Time taken: ' + time_taken + ' minutes ' + '(' + time_hours + ' hours).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1865
    },
    "colab_type": "code",
    "id": "KRuwBEf9RBFm",
    "outputId": "1b1b42a4-b0d7-4297-cf03-352df2f928ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to add hidden layers. Time taken: 9.179115295410156e-07 minutes\n",
      "About to add hidden layers. Time taken: 0.002419443925221761 minutes\n",
      "Layer-adding half done. Time taken: 0.01806903680165609 minutes\n",
      "Layer-adding done, about to compile. Time taken: 0.03499478499094645 minutes\n",
      "Just fitted data aug. Time taken: 0.04456682602564494 minutes\n",
      "Epoch 1/50\n",
      "196/196 [==============================] - 77s 395ms/step - loss: 4.4199 - acc: 0.2323 - val_loss: 6.2138 - val_acc: 0.2710\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 67s 339ms/step - loss: 3.5939 - acc: 0.3551 - val_loss: 3.3106 - val_acc: 0.4309\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 2.9852 - acc: 0.4511 - val_loss: 3.3565 - val_acc: 0.3918\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 2.5742 - acc: 0.5070 - val_loss: 2.6075 - val_acc: 0.4838\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 2.1891 - acc: 0.5607 - val_loss: 2.1780 - val_acc: 0.5561\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.9382 - acc: 0.5941 - val_loss: 1.7903 - val_acc: 0.6252\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.7184 - acc: 0.6348 - val_loss: 2.1353 - val_acc: 0.5667\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.5556 - acc: 0.6610 - val_loss: 1.5077 - val_acc: 0.6671\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.4564 - acc: 0.6811 - val_loss: 1.3619 - val_acc: 0.7125\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.3752 - acc: 0.6975 - val_loss: 1.4195 - val_acc: 0.6887\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.3129 - acc: 0.7136 - val_loss: 1.5453 - val_acc: 0.6325\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.2680 - acc: 0.7259 - val_loss: 1.1537 - val_acc: 0.7580\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.2242 - acc: 0.7349 - val_loss: 1.2057 - val_acc: 0.7447\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 67s 339ms/step - loss: 1.2359 - acc: 0.7346 - val_loss: 1.2909 - val_acc: 0.7204\n",
      "Epoch 15/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.1881 - acc: 0.7499 - val_loss: 1.2271 - val_acc: 0.7201\n",
      "Epoch 16/50\n",
      "196/196 [==============================] - 67s 339ms/step - loss: 1.1666 - acc: 0.7569 - val_loss: 1.0965 - val_acc: 0.7798\n",
      "Epoch 17/50\n",
      "196/196 [==============================] - 67s 339ms/step - loss: 1.1615 - acc: 0.7599 - val_loss: 1.0364 - val_acc: 0.8008\n",
      "Epoch 18/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.1357 - acc: 0.7687 - val_loss: 1.0813 - val_acc: 0.7863\n",
      "Epoch 19/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.1299 - acc: 0.7715 - val_loss: 1.1400 - val_acc: 0.7825\n",
      "Epoch 20/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.1288 - acc: 0.7750 - val_loss: 2.9145 - val_acc: 0.6204\n",
      "Epoch 21/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.1161 - acc: 0.7807 - val_loss: 1.0473 - val_acc: 0.7990\n",
      "Epoch 22/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.1123 - acc: 0.7813 - val_loss: 1.1778 - val_acc: 0.7791\n",
      "Epoch 23/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.1185 - acc: 0.7844 - val_loss: 1.0929 - val_acc: 0.7977\n",
      "Epoch 24/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.1029 - acc: 0.7907 - val_loss: 1.2326 - val_acc: 0.7562\n",
      "Epoch 25/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0952 - acc: 0.7944 - val_loss: 1.1860 - val_acc: 0.7701\n",
      "Epoch 26/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.1104 - acc: 0.7930 - val_loss: 0.9951 - val_acc: 0.8342\n",
      "Epoch 27/50\n",
      "196/196 [==============================] - 67s 340ms/step - loss: 1.0942 - acc: 0.7988 - val_loss: 1.0582 - val_acc: 0.8133\n",
      "Epoch 28/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0892 - acc: 0.8008 - val_loss: 1.1315 - val_acc: 0.7743\n",
      "Epoch 29/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0867 - acc: 0.8040 - val_loss: 1.0239 - val_acc: 0.8256\n",
      "Epoch 30/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0795 - acc: 0.8066 - val_loss: 1.0986 - val_acc: 0.8012\n",
      "Epoch 31/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0867 - acc: 0.8052 - val_loss: 1.0518 - val_acc: 0.8192\n",
      "Epoch 32/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0794 - acc: 0.8094 - val_loss: 1.1651 - val_acc: 0.7854\n",
      "Epoch 33/50\n",
      "196/196 [==============================] - 67s 339ms/step - loss: 1.0645 - acc: 0.8157 - val_loss: 1.0319 - val_acc: 0.8218\n",
      "Epoch 34/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0723 - acc: 0.8113 - val_loss: 1.2425 - val_acc: 0.7593\n",
      "Epoch 35/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0673 - acc: 0.8151 - val_loss: 1.2134 - val_acc: 0.7771\n",
      "Epoch 36/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0684 - acc: 0.8133 - val_loss: 1.0581 - val_acc: 0.8147\n",
      "Epoch 37/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0733 - acc: 0.8134 - val_loss: 0.9785 - val_acc: 0.8451\n",
      "Epoch 38/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0673 - acc: 0.8168 - val_loss: 0.9867 - val_acc: 0.8462\n",
      "Epoch 39/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0675 - acc: 0.8184 - val_loss: 1.1278 - val_acc: 0.7976\n",
      "Epoch 40/50\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 1.0613 - acc: 0.8204 - val_loss: 1.0545 - val_acc: 0.8225\n",
      "Epoch 41/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0659 - acc: 0.8186 - val_loss: 1.0104 - val_acc: 0.8403\n",
      "Epoch 42/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0607 - acc: 0.8221 - val_loss: 0.9930 - val_acc: 0.8419\n",
      "Epoch 43/50\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 1.0578 - acc: 0.8226 - val_loss: 1.1784 - val_acc: 0.7858\n",
      "Epoch 44/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0686 - acc: 0.8195 - val_loss: 1.0608 - val_acc: 0.8186\n",
      "Epoch 45/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0592 - acc: 0.8218 - val_loss: 1.0168 - val_acc: 0.8347\n",
      "Epoch 46/50\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 1.0609 - acc: 0.8234 - val_loss: 1.1064 - val_acc: 0.8092\n",
      "Epoch 47/50\n",
      "196/196 [==============================] - 66s 339ms/step - loss: 1.0618 - acc: 0.8235 - val_loss: 1.0506 - val_acc: 0.8300\n",
      "Epoch 48/50\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 1.0570 - acc: 0.8252 - val_loss: 0.9695 - val_acc: 0.8539\n",
      "Epoch 49/50\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 1.0520 - acc: 0.8259 - val_loss: 1.0085 - val_acc: 0.8387\n",
      "Epoch 50/50\n",
      "196/196 [==============================] - 66s 338ms/step - loss: 1.0587 - acc: 0.8239 - val_loss: 1.0784 - val_acc: 0.8271\n",
      "10000/10000 [==============================] - 8s 821us/step\n",
      "Test loss: 1.0783622783660889\n",
      "Test accuracy: 0.8271\n",
      "************************************************************\n",
      "Time taken: 55.8 minutes (0.9 hours).\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print('About to add hidden layers. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "# Create and instantiate VGG model:\n",
    "model = Sequential()\n",
    "weight_decay = 0.0005\n",
    "input_shape = [32,32,3]\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape=input_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "print('About to add hidden layers. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "print('Layer-adding half done. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "print('Layer-adding done, about to compile. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Try data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_tr)\n",
    "\n",
    "print('Just fitted data aug. Time taken: ' + str((time.time() - start_time)/60) + ' minutes')\n",
    "\n",
    "batch_size=256\n",
    "epochs=50\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_tr, y_tr,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_te, y_te),\n",
    "                        workers=4)\n",
    "\n",
    "# Score trained, data-augmented model:\n",
    "\n",
    "score = model.evaluate(x_te, y_te, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "print('*' * 60)\n",
    "time_taken = '%.1f' % ((time.time() - start_time) / 60)\n",
    "time_hours = '%.1f' % ((time.time() - start_time) / 3600)\n",
    "print('Time taken: ' + time_taken + ' minutes ' + '(' + time_hours + ' hours).')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CIFAR_Using_VGG_16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
